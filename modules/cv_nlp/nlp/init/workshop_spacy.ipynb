{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebbedac6-f152-4189-9e6c-129a9a2a6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "def display(to_display):\n",
    "    print()\n",
    "    print(to_display)\n",
    "    print()\n",
    "\n",
    "def display_sentence_words(sentence, display_sentence = True):\n",
    "    if display_sentence:\n",
    "        display(sentence)\n",
    "    print()\n",
    "    for word in sentence:\n",
    "        print(word.text, word.pos_, word.dep_)\n",
    "    print()\n",
    "\n",
    "def display_entities(sentence):\n",
    "    print()\n",
    "    for entity in sentence.ents:\n",
    "        print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))\n",
    "    print()\n",
    "\n",
    "def display_nouns(sentence):\n",
    "    print()\n",
    "    for noun in sentence.noun_chunks:\n",
    "        print(noun.text)\n",
    "    print()\n",
    "\n",
    "def display_stemming(tokens, stemmer):\n",
    "    print()\n",
    "    for token in tokens:\n",
    "        print(token + ' --> ' + stemmer.stem(token))\n",
    "    print()\n",
    "\n",
    "def display_lemmatization(sentence):\n",
    "    print()\n",
    "    for word in sentence:\n",
    "        print(word.text + '  ===>', word.lemma_)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799ca91e-9bc4-4e3a-bdec-f4445f166a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the core English language model\n",
    "sp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f8262-fb1a-4765-8bf7-2bba974d1bdb",
   "metadata": {},
   "source": [
    "\n",
    "## Basic Functionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3811a12a-8b55-4725-9f87-8109740e8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manchester United is looking to sign a forward for $90 million\n",
      "\n",
      "\n",
      "Manchester PROPN compound\n",
      "United PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "to PART aux\n",
      "sign VERB xcomp\n",
      "a DET det\n",
      "forward NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "90 NUM compound\n",
      "million NUM pobj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a small document using model\n",
    "sentence = sp(u'Manchester United is looking to sign a forward for $90 million')\n",
    "\n",
    "display_sentence_words(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c1bebc4-a4d6-4468-87e6-c924a7aa7c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manchester PROPN compound\n",
      "United PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "to PART aux\n",
      "sign VERB xcomp\n",
      "any DET det\n",
      "forward NOUN advmod\n",
      ". PUNCT punct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_2 = sp(u\"Manchester United isn't looking to sign any forward.\")\n",
    "\n",
    "display_sentence_words(sentence_2, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87ed7b19-8093-43c7-832b-39e7b36659cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?\n",
      "\n",
      "\n",
      "Hello INTJ ROOT\n",
      "from ADP prep\n",
      "Stackabuse PROPN pobj\n",
      ". PUNCT punct\n",
      "The DET det\n",
      "site NOUN ROOT\n",
      "with ADP prep\n",
      "the DET det\n",
      "best ADJ amod\n",
      "Python PROPN compound\n",
      "Tutorials PROPN pobj\n",
      ". PUNCT punct\n",
      "What PRON pobj\n",
      "are AUX aux\n",
      "you PRON nsubj\n",
      "looking VERB ROOT\n",
      "for ADP prep\n",
      "? PUNCT punct\n",
      "\n",
      "The\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "document = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?')\n",
    "\n",
    "display_sentence_words(document)\n",
    "\n",
    "print(document[4])\n",
    "print(document[4].is_sent_start)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce3d93-cb9f-4b18-a0c4-f0210e1dcb31",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aeed94a7-711f-463d-a403-3ce32a9bafc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"They're leaving U.K. for U.S.A.\"\n",
      "\n",
      "\n",
      "\" PUNCT punct\n",
      "They PRON nsubj\n",
      "'re AUX aux\n",
      "leaving VERB ROOT\n",
      "U.K. PROPN dobj\n",
      "for ADP prep\n",
      "U.S.A. PROPN pobj\n",
      "\" PUNCT punct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_3 = sp(u'\"They\\'re leaving U.K. for U.S.A.\"')\n",
    "\n",
    "display_sentence_words(sentence_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5782c4bb-ec97-4463-be9c-aaed83ef0baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\n",
      "\n",
      "\n",
      "Hello INTJ intj\n",
      ", PUNCT punct\n",
      "I PRON nsubj\n",
      "am AUX ROOT\n",
      "non ADJ acomp\n",
      "- ADJ attr\n",
      "vegetarian ADJ attr\n",
      ", PUNCT punct\n",
      "email VERB dep\n",
      "me PRON dative\n",
      "the DET det\n",
      "menu NOUN dobj\n",
      "at ADP prep\n",
      "abc-xyz@gmai.com NOUN pobj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_4 = sp(u\"Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\")\n",
    "\n",
    "display(sentence_4)\n",
    "display_sentence_words(sentence_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69e78076-a07d-4bac-94dc-fa80d48aed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# number of tokens in sentence_4\n",
    "display(len(sentence_4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a72257-8627-40ae-bdfd-8420f5844a13",
   "metadata": {},
   "source": [
    "\n",
    "## Detecting Entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ad34ae1-6de7-4714-b1a6-de06a5e38a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manchester United is looking to sign Harry Kane for $90 million\n",
      "\n",
      "\n",
      "Manchester PROPN compound\n",
      "United PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "to PART aux\n",
      "sign VERB xcomp\n",
      "Harry PROPN compound\n",
      "Kane PROPN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "90 NUM compound\n",
      "million NUM pobj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_5 = sp(u'Manchester United is looking to sign Harry Kane for $90 million')  \n",
    "\n",
    "display_sentence_words(sentence_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d37001f7-15c2-4f66-a73c-e3884eaa4e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manchester United - GPE - Countries, cities, states\n",
      "Harry Kane - PERSON - People, including fictional\n",
      "$90 million - MONEY - Monetary values, including unit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "display_entities(sentence_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558b70d-c211-49db-9672-7d29e22a8fc4",
   "metadata": {},
   "source": [
    "\n",
    "## Detecting Nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdbb73ca-b4eb-47b9-a155-d1642be80a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latest Rumours\n",
      "Manchester United\n",
      "Harry Kane\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_6 = sp(u'Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million')\n",
    "\n",
    "display_nouns(sentence_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358de080-70bf-4d67-8361-069124913a42",
   "metadata": {},
   "source": [
    "\n",
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45086a7b-dd3b-46c1-9f42-9f87259abfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58df8ff1-8415-4f41-9a3c-8e65b44e64a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Porter Stemmer\n",
      "\n",
      "compute --> comput\n",
      "computer --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n",
      "\n",
      "> Snowball Stemmer\n",
      "\n",
      "compute --> comput\n",
      "computer --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "\n",
    "#### Porter Stemmer\n",
    "\n",
    "print()\n",
    "print('> Porter Stemmer')\n",
    "display_stemming(\n",
    "    tokens,\n",
    "    PorterStemmer()\n",
    ")\n",
    "\n",
    "#### Snowball Stemmer\n",
    "\n",
    "print('> Snowball Stemmer')\n",
    "display_stemming(\n",
    "    tokens,\n",
    "    SnowballStemmer(language = 'english')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb72919-0e97-416b-acce-d32c9b5eefb3",
   "metadata": {},
   "source": [
    "\n",
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "184b9ec6-d222-4d07-8f37-9654f2559bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "compute  ===> compute\n",
      "computer  ===> computer\n",
      "computed  ===> compute\n",
      "computing  ===> computing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_7 = sp(u'compute computer computed computing')\n",
    "\n",
    "display_lemmatization(sentence_7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41dd15d4-8191-4055-9d82-5a42021c1b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A  ===> a\n",
      "letter  ===> letter\n",
      "has  ===> have\n",
      "been  ===> be\n",
      "written  ===> write\n",
      ",  ===> ,\n",
      "asking  ===> ask\n",
      "him  ===> he\n",
      "to  ===> to\n",
      "be  ===> be\n",
      "released  ===> release\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_8 = sp(u'A letter has been written, asking him to be released')\n",
    "\n",
    "display_lemmatization(sentence_8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
