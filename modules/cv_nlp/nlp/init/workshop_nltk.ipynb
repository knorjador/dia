{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccf7b97",
   "metadata": {},
   "source": [
    "\n",
    "# Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19703ce-d42b-42b4-b92d-bf83dbe4d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt            \n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, twitter_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14aecb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display(to_display):\n",
    "    print()\n",
    "    print(to_display)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79a5cf",
   "metadata": {},
   "source": [
    "\n",
    "### Data cleansing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6bf760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "natural language processing (nlp) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. the goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text= 'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'\n",
    "\n",
    "text = text.lower() \n",
    "\n",
    "display(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463e7d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data the goal is a computer capable of understanding the contents of documents including the contextual nuances of the language within them the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# remove punctuation\n",
    "\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "display(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3cdd6",
   "metadata": {},
   "source": [
    "\n",
    "### Word Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cf7be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'subfield', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', 'the', 'goal', 'is', 'a', 'computer', 'capable', 'of', 'understanding', 'the', 'contents', 'of', 'documents', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', 'the', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "display(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec18ba3",
   "metadata": {},
   "source": [
    "\n",
    "### Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad98be4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "\n",
      "    > Length with stopwords: 82\n",
      "\n",
      "\n",
      "['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', 'goal', 'computer', 'capable', 'understanding', 'contents', 'documents', 'including', 'contextual', 'nuances', 'language', 'within', 'technology', 'accurately', 'extract', 'information', 'insights', 'contained', 'documents', 'well', 'categorize', 'organize', 'documents']\n",
      "\n",
      "\n",
      "    > Length without stopwords: 47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get stopwords\n",
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "display(english_stopwords)\n",
    "\n",
    "# remove stopwords\n",
    "display(\"    > Length with stopwords: \" + str(len(tokens)))\n",
    "\n",
    "tokens = [word for word in tokens if word not in english_stopwords]\n",
    "\n",
    "display(tokens)\n",
    "display(\"    > Length without stopwords: \" + str(len(tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25588009",
   "metadata": {},
   "source": [
    "\n",
    "### Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca52186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data', 'goal', 'comput', 'capabl', 'understand', 'content', 'document', 'includ', 'contextu', 'nuanc', 'languag', 'within', 'technolog', 'accur', 'extract', 'inform', 'insight', 'contain', 'document', 'well', 'categor', 'organ', 'document']\n",
      "\n",
      "\n",
      "    > Length tokens: 47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(words, lang='english'):\n",
    "    _stopwords = nltk.corpus.stopwords.words(lang)\n",
    "    return [word for word in words if word not in _stopwords]\n",
    "    \n",
    "def stemmize(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "tokens = stemmize(tokens) \n",
    "\n",
    "display(tokens)\n",
    "display(\"    > Length tokens: \" + str(len(tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3610110",
   "metadata": {},
   "source": [
    "\n",
    "### Twitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d270188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('twitter_samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "979b2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "061ed30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92m@aaronbethunee I'm sofa surfing :) cunt\n",
      "\n",
      "\n",
      "\u001b[91m@thebodycoach Joe I'm sick can you come round and make me soup ? :(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print positive in greeen\n",
    "display('\\033[92m' + all_positive_tweets[random.randint(0, 5000)])\n",
    "\n",
    "# print negative in red\n",
    "display('\\033[91m' + all_negative_tweets[random.randint(0, 5000)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
